{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install pandas numpy beautifulsoup4 bs64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import bs64\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"An.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "data = [(a[\"href\"], a.text) for a in soup.find_all(\"a\", href=True)]\n",
    "with open(\"links.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"href\", \"text\"])\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = '''\n",
    "<a href=\"http://simplyhired.com\"> simplyhired.com</a>\n",
    "<a href=\"http://jobspresso.co\">jobspresso.co</a>3. Stack Overflow Jobs\n",
    "<a href=\"http://stackoverflow.com\">stackoverflow.com</a>4. Outsourcely\n",
    "<a href=\"http://outsourcely.com\">outsourcely.com</a>5. Toptal\n",
    "<a href=\"http://toptal.com\">toptal.com</a>6. Skip The Drive\n",
    "<a href=\"http://skipthechive.com\">skipthechive.com</a>7. NoDesk\n",
    "<a href=\"http://nodesk.co\">nodesk.co</a>8. RemoteHabits\n",
    "<a href=\"http://remotehabits.com\">remotehabits.com</a>9. Remotive\n",
    "<a href=\"http://remotive.com\">remotive.com</a>10. Remote4Me\n",
    "<a href=\"http://remote4me.com\">remote4me.com</a>11. Pangian\n",
    "<a href=\"http://pangian.com\">pangian.com</a>12. Remotees\n",
    "<a href=\"http://remotees.com\">remotees.com</a>13. Europe Remotely\n",
    "<a href=\"http://europeremotely.com\">europeremotely.com</a>14. Remote OK Europe\n",
    "<a href=\"https://lnkd.in/gr4C-mjp\">https://lnkd.in/gr4C-mjp</a>15. Remote of\n",
    "<a href=\"https://lnkd.in/ghrA_z9u\">https://lnkd.in/ghrA_z9u</a>16. FlexJobs\n",
    "<a href=\"http://flexjobs.com\">flexjobs.com</a>17\n",
    "<a href=\"http://Remote.co\">Remote.co</a>\n",
    "<a href=\"http://remote.co\">remote.co</a>18. We Work Remotely\n",
    "<a href=\"http://weworkremotely.com\">weworkremotely.com</a>19. RemoteOK\n",
    "<a href=\"http://remoteok.com\">remoteok.com</a>20. AngelList\n",
    "<a href=\"http://angel.co\">angel.co</a>21. Linkedin\n",
    "<a href=\"http://linkedin.com\">linkedin.com</a>22. Outsourcely\n",
    "<a href=\"http://outsourcely.com\">outsourcely.com</a>23. Freelancer\n",
    "<a href=\"http://freelancer.com\">freelancer.com</a>24. Working Nomads\n",
    "<a href=\"http://workingnomads.com\">workingnomads.com</a>25. Virtual Vocations\n",
    "<a href=\"http://virtualvocations.com\">virtualvocations.com</a>26. Wellfound\n",
    "<a href=\"https://wellfound.com\">https://wellfound.com</a>27.Remote freelance\n",
    "<a href=\"http://remotefreelance.com\">remotefreelance.com</a>28. Remote\n",
    "<a href=\"https://lnkd.in/gS2nRtV3\">https://lnkd.in/gS2nRtV3</a>29.\n",
    "<a href=\"http://jobspresso.co\">jobspresso.co</a>30. SimplyHired\n",
    "<a href=\"http://simplyhired.com\">simplyhired.com</a>'''\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "data = [(a[\"href\"], a.text) for a in soup.find_all(\"a\", href=True)]\n",
    "with open(\"links.csv\", \"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_df = pd.read_csv('links.csv')\n",
    "a_links = a_df['Link'].dropna().tolist()\n",
    "crawl_df = pd.read_excel('Crawl.xlsx')\n",
    "crawl_websites = crawl_df['Website'].dropna().tolist()\n",
    "missing_links = [link for link in a_links if link not in crawl_websites]\n",
    "\n",
    "for link in missing_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"links.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "duplicates = df[df.duplicated(keep=False)]\n",
    "if not duplicates.empty:\n",
    "    print(\"Repeat\")\n",
    "    display(duplicates)\n",
    "else:\n",
    "    print(\"Not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Links.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.drop_duplicates()\n",
    "df.to_csv(file_path, index=False)\n",
    "print(\"Updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Crawl.xlsx\")\n",
    "\n",
    "if \"Website\" in df.columns:\n",
    "    duplicates = df[\"Website\"].duplicated(keep=False)\n",
    "    duplicate_values = df.loc[duplicates, \"Website\"].value_counts()\n",
    "    if not duplicate_values.empty:\n",
    "        print(\"Repeat list : \")\n",
    "        print(duplicate_values)\n",
    "    else:\n",
    "        print(\"Rep\")\n",
    "else:\n",
    "    print(\"NotRepeat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Crawl.xlsx\")\n",
    "\n",
    "if \"Website\" in df.columns:\n",
    "    df = df.drop_duplicates(subset=[\"Website\"], keep='first')\n",
    "    df.to_excel(\"Crawl_Cleaned.xlsx\", index=False)  \n",
    "    print(\"Made\")\n",
    "else:\n",
    "    print(\"Not Found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regx for replace all data in \"...\" to other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^.*?\"([^\"\\n]*)\".*$    ->    $1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regx for replace all data after main domain like \"www.sample.com/..........\" to nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^((?:https?:\\/\\/)?(?:www\\.)?[^/]+)(/[^/].+)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
